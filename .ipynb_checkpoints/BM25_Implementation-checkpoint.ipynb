{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde43208",
   "metadata": {},
   "source": [
    "# Практическое задание 5: Реализация модели BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1993d2",
   "metadata": {},
   "source": [
    "\n",
    "## Цель задания\n",
    "Научиться реализовывать модель BM25 (Best Matching 25), используемую для оценки релевантности документов на основе текстовых запросов.\n",
    "\n",
    "## Теоретическая часть\n",
    "BM25 — это вероятностная модель ранжирования, которая улучшает классический TF-IDF за счет учета длины документов и дополнительных параметров.\n",
    "\n",
    "### Формула для вычисления BM25:\n",
    "\n",
    "\\[\n",
    "BM25(t, d) = IDF(t) \\cdot \\frac{TF(t, d) \\cdot (k_1 + 1)}{TF(t, d) + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{avgdl})}\n",
    "\\]\n",
    "\n",
    "Где:\n",
    "- **TF(t,d)** — частота термина t в документе d.\n",
    "- **|d|** — длина документа d.\n",
    "- **avgdl** — средняя длина документов в корпусе.\n",
    "- **k1** и **b** — гиперпараметры модели (обычно k1=1.5, b=0.75).\n",
    "- **IDF(t)** — взвешивание термина:\n",
    "\n",
    "\\[\n",
    "IDF(t) = \\ln\\left( \\frac{N - n_t + 0.5}{n_t + 0.5} + 1 \\right)\n",
    "\\]\n",
    "\n",
    "Где:\n",
    "- **N** — общее количество документов.\n",
    "- **n_t** — число документов, содержащих термин t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea5fea",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c6ee19d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T09:43:37.524163Z",
     "start_time": "2025-02-27T09:43:35.696079Z"
    }
   },
   "source": [
    "\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Загрузка необходимых ресурсов NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Загружаем NLP-модель для лемматизации\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Исходные документы\n",
    "documents = [\n",
    "    \"Natural language processing enables machines to understand human language.\",\n",
    "    \"Boolean retrieval is a basic model in information retrieval.\",\n",
    "    \"Language models are essential for processing and analyzing text.\",\n",
    "    \"Understanding Boolean operators is crucial for search engines.\"\n",
    "]\n",
    "\n",
    "# Функция предобработки текста\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    return [word for word in lemmatized_tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Применяем предобработку\n",
    "processed_documents = [preprocess(doc) for doc in documents]\n",
    "processed_documents\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikei/PycharmProjects/IRIE_practice_5/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /Users/aikei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/aikei/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstopwords\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Загружаем NLP-модель для лемматизации\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men_core_web_sm\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Исходные документы\u001B[39;00m\n\u001B[1;32m     15\u001B[0m documents \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNatural language processing enables machines to understand human language.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBoolean retrieval is a basic model in information retrieval.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLanguage models are essential for processing and analyzing text.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnderstanding Boolean operators is crucial for search engines.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     20\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/IRIE_practice_5/.venv/lib/python3.9/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/IRIE_practice_5/.venv/lib/python3.9/site-packages/spacy/util.py:472\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[0;32m--> 472\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[0;31mOSError\u001B[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "33097b0b",
   "metadata": {},
   "source": [
    "## Часть 2. Вычисление параметров BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import log\n",
    "\n",
    "def compute_idf(corpus):\n",
    "    N = len(corpus)\n",
    "    idf = {}\n",
    "    for doc in corpus:\n",
    "        for word in set(doc):\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "    for word, freq in idf.items():\n",
    "        idf[word] = log((N - freq + 0.5) / (freq + 0.5) + 1)\n",
    "    return idf\n",
    "\n",
    "def compute_tf(doc):\n",
    "    tf = {}\n",
    "    for word in doc:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    return tf\n",
    "\n",
    "# Вычисление параметров\n",
    "doc_lengths = [len(doc) for doc in processed_documents]\n",
    "avgdl = sum(doc_lengths) / len(doc_lengths)\n",
    "idf = compute_idf(processed_documents)\n",
    "\n",
    "idf, avgdl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d238c",
   "metadata": {},
   "source": [
    "## Часть 3. Реализация BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d45301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bm25_score(query, doc, idf, k1=1.5, b=0.75):\n",
    "    tf = compute_tf(doc)\n",
    "    score = 0\n",
    "    for term in query:\n",
    "        if term in doc:\n",
    "            term_tf = tf[term]\n",
    "            numerator = term_tf * (k1 + 1)\n",
    "            denominator = term_tf + k1 * (1 - b + b * (len(doc) / avgdl))\n",
    "            score += idf.get(term, 0) * (numerator / denominator)\n",
    "    return score\n",
    "\n",
    "# Тестирование на запросе\n",
    "query = preprocess(\"language models retrieval\")\n",
    "scores = [bm25_score(query, doc, idf) for doc in processed_documents]\n",
    "\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cdc934",
   "metadata": {},
   "source": [
    "## Часть 4. Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    \"natural language processing\",\n",
    "    \"Boolean retrieval\",\n",
    "    \"models text\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    processed_query = preprocess(query)\n",
    "    scores = [bm25_score(processed_query, doc, idf) for doc in processed_documents]\n",
    "    sorted_results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Запрос: {query}\")\n",
    "    for rank, (doc_index, score) in enumerate(sorted_results):\n",
    "        print(f\"{rank + 1}. Документ {doc_index + 1}: {documents[doc_index]} (счет: {score:.4f})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26673c1",
   "metadata": {},
   "source": [
    "## Часть 5. Улучшения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda7b53",
   "metadata": {},
   "source": [
    "\n",
    "### Внесенные улучшения:\n",
    "1. **Лемматизация**: Добавлена поддержка лемматизации с использованием spaCy.\n",
    "2. **Масштабируемость**: Для тестирования можно использовать больший корпус текстов.\n",
    "3. **Оптимизация параметров**: Можно изменять значения k1 и b для улучшения качества ранжирования.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
